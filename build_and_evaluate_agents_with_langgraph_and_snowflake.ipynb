{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi-Agent Supervisor Workflow with LangGraph and Snowflake\n",
    "\n",
    "This notebook demonstrates how to build a **multi-agent supervisor architecture** using LangGraph and Snowflake Cortex Agents. The workflow consists of:\n",
    "\n",
    "- **Supervisor**: An AI coordinator that routes queries to specialized agents and synthesizes their responses\n",
    "- **Content Agent**: Handles customer feedback, sentiment analysis, and communication intelligence\n",
    "- **Data Analyst Agent**: Handles customer behavior, business metrics, and predictive analytics\n",
    "- **Research Agent**: Handles market intelligence, strategic analysis, and competitive insights\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "START ‚Üí Supervisor (Route) ‚Üí Specialized Agent ‚Üí Supervisor (Synthesize) ‚Üí END\n",
    "```\n",
    "\n",
    "The supervisor makes two passes:\n",
    "1. **Routing Pass**: Analyzes the query and routes to the appropriate agent\n",
    "2. **Synthesis Pass**: Combines agent output into an executive summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Dependencies\n",
    "\n",
    "First, let's import all the necessary libraries. We'll need:\n",
    "- **LangChain Core**: For message types and prompt templates\n",
    "- **LangChain Snowflake**: For Snowflake-specific integrations (ChatSnowflake, SnowflakeCortexAgent)\n",
    "- **LangGraph**: For building the stateful workflow graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from langgraph.graph.message import MessagesState\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Fix for langchain_snowflake import compatibility issue\n",
    "# Tool class moved from langchain.tools to langchain_core.tools in newer LangChain\n",
    "import langchain.tools\n",
    "from langchain_core.tools import Tool\n",
    "langchain.tools.Tool = Tool  # Shim for backwards compatibility\n",
    "\n",
    "# Snowflake imports\n",
    "from langchain_snowflake import ChatSnowflake, SnowflakeCortexAgent, create_session_from_env\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Utility imports\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Literal\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Workflow State\n",
    "\n",
    "LangGraph uses a **state** object that flows through the graph. We'll use the built-in `MessagesState` which provides:\n",
    "- A `messages` list that accumulates all messages in the conversation\n",
    "- Automatic message deduplication and ordering\n",
    "\n",
    "The state is shared across all nodes and gets updated as the workflow progresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend MessagesState to include execution plan tracking\n",
    "class State(MessagesState):\n",
    "    \"\"\"Extended state that tracks execution plan and progress.\n",
    "    \n",
    "    Designed for efficient single-pass execution:\n",
    "    - Plan is created ONCE and never modified during execution\n",
    "    - Agents chain directly to each other without supervisor re-entry\n",
    "    - Errors are aggregated, not cascaded\n",
    "    \"\"\"\n",
    "    plan: Optional[Dict] = None  # The supervisor's explicit plan (immutable after creation)\n",
    "    current_step: int = 0  # Current step being executed in the plan\n",
    "    agent_outputs: Dict = {}  # Accumulated outputs from all agents (avoids message parsing)\n",
    "    execution_errors: List = []  # Aggregated errors (not per-chunk)\n",
    "    planning_complete: bool = False  # Flag to skip re-planning\n",
    "    \n",
    "print(\"‚úÖ State defined with efficiency optimizations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Snowflake Session\n",
    "\n",
    "We need to establish a connection to Snowflake. The `create_session_from_env()` function reads credentials from environment variables:\n",
    "- `SNOWFLAKE_ACCOUNT`\n",
    "- `SNOWFLAKE_USER`\n",
    "- `SNOWFLAKE_PASSWORD`\n",
    "- `SNOWFLAKE_DATABASE`\n",
    "- `SNOWFLAKE_SCHEMA`\n",
    "- `SNOWFLAKE_WAREHOUSE`\n",
    "\n",
    "Make sure you have a `.env` file with these variables set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake session from environment variables\n",
    "try:\n",
    "    session = create_session_from_env()\n",
    "    \n",
    "    # Get database and schema, stripping any quotes that Snowflake might add\n",
    "    current_database = session.get_current_database().strip('\"')\n",
    "    current_schema = session.get_current_schema().strip('\"')\n",
    "    \n",
    "    # IMPORTANT: Ensure warehouse is set - required for Cortex Analyst tools\n",
    "    warehouse = os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH')\n",
    "    session.sql(f\"USE WAREHOUSE {warehouse}\").collect()\n",
    "    current_warehouse = session.get_current_warehouse()\n",
    "    if current_warehouse:\n",
    "        current_warehouse = current_warehouse.strip('\"')\n",
    "    \n",
    "    print(\"‚úÖ Snowflake session created successfully!\")\n",
    "    print(f\"   Database: {current_database}\")\n",
    "    print(f\"   Schema: {current_schema}\")\n",
    "    print(f\"   Warehouse: {current_warehouse}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Snowflake connection failed: {e}\")\n",
    "    print(\"   Please check your .env file and Snowflake credentials\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize the Supervisor Model\n",
    "\n",
    "The **Supervisor** is the brain of our multi-agent system. It uses Snowflake's Cortex LLM service (`ChatSnowflake`) to:\n",
    "1. Analyze incoming queries\n",
    "2. Route them to the appropriate specialized agent\n",
    "3. Synthesize the agent's response into an executive summary\n",
    "\n",
    "We use `claude-4-sonnet` with low temperature (0.1) for consistent, deterministic routing decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the supervisor model using Snowflake Cortex\n",
    "supervisor_model = ChatSnowflake(\n",
    "    session=session,\n",
    "    model=\"claude-4-sonnet\",  # Claude 4 Sonnet via Snowflake Cortex\n",
    "    temperature=0.1,          # Low temperature for consistent routing\n",
    "    max_tokens=2000           # Limit response length\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Supervisor model initialized!\")\n",
    "print(f\"   Model: claude-4-sonnet\")\n",
    "print(f\"   Temperature: 0.1 (deterministic)\")\n",
    "print(f\"   Max tokens: 2000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Specialized Cortex Agents\n",
    "\n",
    "Now we initialize our three specialized **Snowflake Cortex Agents**. These agents are pre-configured in Snowflake with access to specific tools, data sources, and instructions.\n",
    "\n",
    "Each agent is specialized for a different domain:\n",
    "| Agent | Specialization |\n",
    "|-------|----------------|\n",
    "| **CONTENT_AGENT** | Customer feedback, sentiment analysis, support tickets |\n",
    "| **DATA_ANALYST_AGENT** | Metrics, behavior patterns, churn prediction, analytics |\n",
    "| **RESEARCH_AGENT** | Market research, competitive analysis, strategic insights |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent configuration - agents are stored in the SNOWFLAKE_INTELLIGENCE.AGENTS schema\n",
    "AGENT_DATABASE = \"SNOWFLAKE_INTELLIGENCE\"\n",
    "AGENT_SCHEMA = \"AGENTS\"\n",
    "AGENT_WAREHOUSE = os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH')\n",
    "\n",
    "# Initialize Content Agent - Customer feedback and sentiment specialist\n",
    "content_agent = SnowflakeCortexAgent(\n",
    "    session=session,\n",
    "    name=\"CONTENT_AGENT\",\n",
    "    database=AGENT_DATABASE,\n",
    "    schema=AGENT_SCHEMA,\n",
    "    warehouse=AGENT_WAREHOUSE,  # Required for tool execution\n",
    "    description=\"Customer feedback, sentiment analysis, and communication intelligence specialist\",\n",
    ")\n",
    "print(\"‚úÖ CONTENT_AGENT initialized\")\n",
    "\n",
    "# Initialize Data Analyst Agent - Metrics and analytics specialist\n",
    "data_analyst_agent = SnowflakeCortexAgent(\n",
    "    session=session,\n",
    "    name=\"DATA_ANALYST_AGENT\",\n",
    "    database=AGENT_DATABASE,\n",
    "    schema=AGENT_SCHEMA,\n",
    "    warehouse=AGENT_WAREHOUSE,  # Required for Cortex Analyst text-to-SQL\n",
    "    description=\"Customer behavior, business metrics, and predictive analytics specialist\",\n",
    ")\n",
    "print(\"‚úÖ DATA_ANALYST_AGENT initialized\")\n",
    "\n",
    "# Initialize Research Agent - Market intelligence specialist\n",
    "research_agent = SnowflakeCortexAgent(\n",
    "    session=session,\n",
    "    name=\"RESEARCH_AGENT\",\n",
    "    database=AGENT_DATABASE,\n",
    "    schema=AGENT_SCHEMA,\n",
    "    warehouse=AGENT_WAREHOUSE,  # Required for Cortex Analyst text-to-SQL\n",
    "    description=\"Market intelligence, strategic analysis, and competitive insights specialist\",\n",
    ")\n",
    "print(\"‚úÖ RESEARCH_AGENT initialized\")\n",
    "\n",
    "print(f\"\\nüìç All agents loaded from: {AGENT_DATABASE}.{AGENT_SCHEMA}\")\n",
    "print(f\"üìç Using warehouse: {AGENT_WAREHOUSE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Supervisor Prompts\n",
    "\n",
    "The supervisor uses two prompts depending on its current task:\n",
    "\n",
    "### 1. Planning Prompt\n",
    "Creates an **explicit, detailed execution plan** before any agents are called. This ensures transparency, accountability, and methodical execution. The plan includes:\n",
    "\n",
    "| Element | Description |\n",
    "|---------|-------------|\n",
    "| **Plan Summary** | Concise description of the analytical approach |\n",
    "| **Steps** | Ordered list of agent calls with detailed specifications |\n",
    "| **Step Dependencies** | How data flows between steps |\n",
    "| **Combination Strategy** | Methodology for synthesizing all results |\n",
    "| **Expected Final Output** | What the final deliverable must contain |\n",
    "\n",
    "Each step in the plan specifies:\n",
    "- **Agent**: Which specialized agent to call\n",
    "- **Purpose**: Why this agent is needed\n",
    "- **Tools to Use**: Specific tools (Cortex Search, Churn Model, etc.)\n",
    "- **Data Sources**: Tables and data to query\n",
    "- **Methodology**: Step-by-step approach for the agent\n",
    "- **Specific Queries**: Questions the agent must answer\n",
    "- **Metrics to Collect**: KPIs and data points to gather\n",
    "- **Expected Output**: What the agent should deliver\n",
    "- **Success Criteria**: How to verify the step succeeded\n",
    "- **Dependencies**: Which previous steps this depends on\n",
    "\n",
    "### 2. Synthesis Prompt\n",
    "Used after all agents complete to combine findings into a comprehensive executive summary. The synthesis:\n",
    "- Follows the combination strategy from the plan\n",
    "- Verifies success criteria for each step\n",
    "- Correlates findings across agents using step dependencies\n",
    "- Meets the expected final output requirements\n",
    "- Includes quantitative analysis, risk assessment, and prioritized recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planning Prompt - Creates detailed, actionable execution plans\n",
    "# EFFICIENCY OPTIMIZATIONS:\n",
    "# - Emphasizes consolidated SQL queries (single aggregation vs multiple separate calls)\n",
    "# - Clear tool separation based on actual agent configuration\n",
    "# - Plan is created ONCE and executed linearly without re-planning\n",
    "planning_prompt = \"\"\"\n",
    "You are an Executive AI Assistant supervisor. Create DETAILED, ACTIONABLE execution plans.\n",
    "\n",
    "**CRITICAL EFFICIENCY RULES:**\n",
    "1. **ONE PLAN, ONE EXECUTION** - Create the plan once. It will NOT be modified during execution.\n",
    "2. **CONSOLIDATE QUERIES** - Use single SQL aggregations instead of multiple separate queries.\n",
    "   - BAD: Separate queries for COUNT, AVG, SUM, etc.\n",
    "   - GOOD: One query with SELECT COUNT(*), AVG(col), SUM(col), ... GROUP BY ...\n",
    "3. **MINIMIZE AGENT CALLS** - Only use multiple agents when their specialized tools are needed.\n",
    "4. **USE THE RIGHT TOOL** - Each agent has specific tools for specific purposes.\n",
    "\n",
    "**AVAILABLE AGENTS AND TOOLS:**\n",
    "\n",
    "| Agent | Tools | Data Access | Best For |\n",
    "|-------|-------|-------------|----------|\n",
    "| CONTENT_AGENT | CUSTOMER_FEEDBACK_SEARCH (cortex_search), CUSTOMER_CONTENT_ANALYZER (UDF) | SUPPORT_TICKETS_SEARCH index | Semantic search for complaints/feedback, sentiment analysis on specific customers |\n",
    "| DATA_ANALYST_AGENT | BUSINESS_INTELLIGENCE_ANALYST (cortex_analyst), CUSTOMER_BEHAVIOR_ANALYZER (UDF) | CUSTOMER_BEHAVIOR_ANALYST semantic view (CUSTOMERS, USAGE_EVENTS, SUPPORT_TICKETS, CHURN_EVENTS) | Usage patterns, churn analysis, engagement metrics, behavior trends |\n",
    "| RESEARCH_AGENT | STRATEGIC_MARKET_ANALYST (cortex_analyst), CUSTOMER_SEGMENT_INTELLIGENCE (UDF) | STRATEGIC_RESEARCH_ANALYST semantic view (CUSTOMERS, USAGE_EVENTS, SUPPORT_TICKETS, CHURN_EVENTS) | Market intelligence, industry analysis, revenue/CLV analysis, strategic insights |\n",
    "\n",
    "**KEY DATA FIELDS AVAILABLE:**\n",
    "- CUSTOMERS: customer_id, company_size, industry, plan_type, status, signup_date, monthly_revenue\n",
    "- USAGE_EVENTS: event_id, customer_id, feature_used, event_date, session_duration_minutes, actions_count\n",
    "- SUPPORT_TICKETS: ticket_id, customer_id, category, priority, status, created_date, resolution_time_hours, satisfaction_score\n",
    "- CHURN_EVENTS: churn_id, customer_id, churn_reason, churn_date, days_since_signup, final_monthly_revenue\n",
    "\n",
    "**AGENT SELECTION GUIDE:**\n",
    "- Need to SEARCH ticket text for specific issues ‚Üí CONTENT_AGENT (cortex_search)\n",
    "- Need to ANALYZE specific customers' sentiment ‚Üí CONTENT_AGENT (UDF)\n",
    "- Need aggregate BEHAVIOR metrics (usage, sessions, engagement) ‚Üí DATA_ANALYST_AGENT\n",
    "- Need STRATEGIC analysis (CLV, market share, industry trends) ‚Üí RESEARCH_AGENT\n",
    "- Need to analyze specific customer segments strategically ‚Üí RESEARCH_AGENT (UDF)\n",
    "\n",
    "**NEVER DO:**\n",
    "- Issue separate queries for each metric (consolidate into ONE query)\n",
    "- Re-plan or update the plan mid-execution\n",
    "- Use multiple agents when one can answer the question\n",
    "\n",
    "**JSON Response Format:**\n",
    "{{\n",
    "    \"plan_summary\": \"[AGENT(s)] will use [TOOL(s)] to query [DATA_SOURCE(s)] for [GOAL]\",\n",
    "    \"total_steps\": <number>,\n",
    "    \"steps\": [\n",
    "        {{\n",
    "            \"step_number\": 1,\n",
    "            \"agent\": \"AGENT_NAME\",\n",
    "            \"tool\": \"TOOL_NAME\",\n",
    "            \"data_source\": \"Semantic view or search index name\",\n",
    "            \"purpose\": \"Specific analytical task\",\n",
    "            \"consolidated_query\": \"SINGLE query/search that gets ALL needed data for this step\",\n",
    "            \"expected_output\": \"Specific columns/fields to return\",\n",
    "            \"uses_data_from\": [],\n",
    "            \"next_agent\": \"AGENT_NAME or null if last step\"\n",
    "        }}\n",
    "    ],\n",
    "    \"combination_strategy\": \"How results will be joined/synthesized\",\n",
    "    \"expected_final_output\": \"Final deliverable specification\"\n",
    "}}\n",
    "\n",
    "**Example - EFFICIENT Single Agent (Revenue by industry):**\n",
    "\n",
    "Query: \"What industries have the highest customer lifetime value?\"\n",
    "{{\n",
    "    \"plan_summary\": \"RESEARCH_AGENT will use STRATEGIC_MARKET_ANALYST to analyze revenue by industry\",\n",
    "    \"total_steps\": 1,\n",
    "    \"steps\": [\n",
    "        {{\n",
    "            \"step_number\": 1,\n",
    "            \"agent\": \"RESEARCH_AGENT\",\n",
    "            \"tool\": \"STRATEGIC_MARKET_ANALYST\",\n",
    "            \"data_source\": \"STRATEGIC_RESEARCH_ANALYST semantic view\",\n",
    "            \"purpose\": \"Get revenue statistics aggregated by industry in a SINGLE query\",\n",
    "            \"consolidated_query\": \"Aggregate monthly_revenue and customer counts by industry, ordered by total revenue\",\n",
    "            \"expected_output\": \"industry, total_revenue, avg_revenue, customer_count\",\n",
    "            \"uses_data_from\": [],\n",
    "            \"next_agent\": null\n",
    "        }}\n",
    "    ],\n",
    "    \"combination_strategy\": \"Present ranked results directly\",\n",
    "    \"expected_final_output\": \"Ranked table of industries by revenue with customer counts\"\n",
    "}}\n",
    "\n",
    "**Example - EFFICIENT Multi-Agent (churn risk + complaints):**\n",
    "\n",
    "Query: \"Assess churn risk for customers complaining about API issues\"\n",
    "{{\n",
    "    \"plan_summary\": \"CONTENT_AGENT searches tickets for API complaints, then DATA_ANALYST_AGENT analyzes churn patterns\",\n",
    "    \"total_steps\": 2,\n",
    "    \"steps\": [\n",
    "        {{\n",
    "            \"step_number\": 1,\n",
    "            \"agent\": \"CONTENT_AGENT\",\n",
    "            \"tool\": \"CUSTOMER_FEEDBACK_SEARCH\",\n",
    "            \"data_source\": \"SUPPORT_TICKETS_SEARCH index\",\n",
    "            \"purpose\": \"Find API-related complaints and extract customer IDs\",\n",
    "            \"consolidated_query\": \"Semantic search: 'API error issue problem integration failure'\",\n",
    "            \"expected_output\": \"customer_ids and ticket summaries from matching tickets\",\n",
    "            \"uses_data_from\": [],\n",
    "            \"next_agent\": \"DATA_ANALYST_AGENT\"\n",
    "        }},\n",
    "        {{\n",
    "            \"step_number\": 2,\n",
    "            \"agent\": \"DATA_ANALYST_AGENT\",\n",
    "            \"tool\": \"BUSINESS_INTELLIGENCE_ANALYST\",\n",
    "            \"data_source\": \"CUSTOMER_BEHAVIOR_ANALYST semantic view\",\n",
    "            \"purpose\": \"Get behavior and churn metrics for identified customers in ONE query\",\n",
    "            \"consolidated_query\": \"Query customers, usage patterns, and churn events for the identified customer_ids\",\n",
    "            \"expected_output\": \"customer_id, status, monthly_revenue, usage metrics, churn indicators\",\n",
    "            \"uses_data_from\": [1],\n",
    "            \"next_agent\": null\n",
    "        }}\n",
    "    ],\n",
    "    \"combination_strategy\": \"Join ticket data with behavior metrics by customer_id\",\n",
    "    \"expected_final_output\": \"Risk-ranked list with complaint summary and churn indicators\"\n",
    "}}\n",
    "\n",
    "**Query:** {input}\n",
    "\n",
    "**RESPOND WITH ONLY THE JSON - Plan will be executed exactly as specified, no modifications.**\n",
    "\"\"\"\n",
    "\n",
    "planning_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", planning_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Planning prompt updated - reflects actual agent tools and data sources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesis Prompt - Combines agent results into a clear, confident answer\n",
    "synthesis_prompt = \"\"\"\n",
    "You are an Executive AI Assistant synthesizing agent results into a clear answer.\n",
    "\n",
    "**Original Question**: {question}\n",
    "\n",
    "**Plan Summary**: {plan_summary}\n",
    "\n",
    "**Agent Results**:\n",
    "{agent_outputs}\n",
    "\n",
    "**Your Task**: Provide a clear, confident answer to the question using the data returned.\n",
    "\n",
    "**Synthesis Guidelines:**\n",
    "1. **Lead with the answer** - Start with the key finding that answers the question\n",
    "2. **Present what you learned** - Use the actual data returned by agents\n",
    "3. **Be confident** - Don't apologize for what wasn't analyzed\n",
    "4. **Be concise** - Executive summary style, not exhaustive reports\n",
    "5. **Add value** - Include actionable insights based on the data\n",
    "\n",
    "**DO NOT:**\n",
    "- List \"missing data\" or \"incomplete analysis\"\n",
    "- Mark steps as \"met/not met\"\n",
    "- Apologize for limitations\n",
    "- Add disclaimers about data gaps\n",
    "- Suggest the analysis is incomplete if you have enough to answer the question\n",
    "\n",
    "**Response Format:**\n",
    "\n",
    "## Summary\n",
    "[Direct answer to the question in 2-3 sentences with key metrics]\n",
    "\n",
    "## Key Findings\n",
    "[3-5 bullet points of important insights from the data]\n",
    "\n",
    "## Recommendations\n",
    "[2-3 actionable next steps based on findings]\n",
    "\n",
    "Keep the response focused and actionable. If the data answers the question, present it confidently.\n",
    "\"\"\"\n",
    "\n",
    "synthesis_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", synthesis_prompt),\n",
    "    (\"human\", \"Synthesize the agent results into a clear answer to the original question.\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Helper Functions\n",
    "\n",
    "We need several helper functions to work with the message state:\n",
    "\n",
    "1. **`get_latest_human_message`**: Extracts the user's query from the message list\n",
    "2. **`has_agent_response`**: Checks if any specialized agent has already responded\n",
    "3. **`get_agent_output`**: Retrieves the agent's response for synthesis\n",
    "\n",
    "These helpers make our node functions cleaner and handle various message formats (including LangGraph Studio's format).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_human_message(messages: List[BaseMessage]) -> str:\n",
    "    \"\"\"Extract the latest human message content from the message list.\"\"\"\n",
    "    if not messages:\n",
    "        return \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            content = msg.content\n",
    "            if isinstance(content, list):\n",
    "                for item in content:\n",
    "                    if isinstance(item, dict) and item.get(\"type\") == \"text\":\n",
    "                        return item.get(\"text\", \"\")\n",
    "                    elif isinstance(item, str):\n",
    "                        return item\n",
    "            return str(content)\n",
    "        if isinstance(msg, dict):\n",
    "            if msg.get(\"type\") == \"human\" or msg.get(\"role\") == \"user\":\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                if isinstance(content, list):\n",
    "                    for item in content:\n",
    "                        if isinstance(item, dict) and item.get(\"type\") == \"text\":\n",
    "                            return item.get(\"text\", \"\")\n",
    "                return str(content)\n",
    "    return \"\"\n",
    "\n",
    "# Agent names for identification\n",
    "AGENT_NAMES = [\"CONTENT_AGENT\", \"DATA_ANALYST_AGENT\", \"RESEARCH_AGENT\"]\n",
    "\n",
    "def has_plan(state) -> bool:\n",
    "    \"\"\"Check if an execution plan has been created.\"\"\"\n",
    "    return state.get(\"plan\") is not None and state.get(\"planning_complete\", False)\n",
    "\n",
    "def get_current_step(state):\n",
    "    \"\"\"Get the current step from the execution plan.\"\"\"\n",
    "    plan = state.get(\"plan\")\n",
    "    current_step_idx = state.get(\"current_step\", 0)\n",
    "    if plan and \"steps\" in plan:\n",
    "        steps = plan[\"steps\"]\n",
    "        if current_step_idx < len(steps):\n",
    "            return steps[current_step_idx]\n",
    "    return None\n",
    "\n",
    "def is_plan_complete(state) -> bool:\n",
    "    \"\"\"Check if all steps in the plan have been executed.\"\"\"\n",
    "    plan = state.get(\"plan\")\n",
    "    current_step_idx = state.get(\"current_step\", 0)\n",
    "    if plan and \"steps\" in plan:\n",
    "        return current_step_idx >= len(plan[\"steps\"])\n",
    "    return True\n",
    "\n",
    "def get_all_agent_outputs(state) -> Dict[str, str]:\n",
    "    \"\"\"Get all agent outputs from state (uses dedicated field for efficiency).\n",
    "    Falls back to message parsing if agent_outputs not populated.\"\"\"\n",
    "    # Prefer dedicated state field (more efficient)\n",
    "    if state.get(\"agent_outputs\"):\n",
    "        return state.get(\"agent_outputs\", {})\n",
    "    \n",
    "    # Fallback to message parsing\n",
    "    outputs = {}\n",
    "    messages = state.get(\"messages\", [])\n",
    "    for msg in messages:\n",
    "        if hasattr(msg, 'name') and msg.name in AGENT_NAMES:\n",
    "            content = msg.content if hasattr(msg, 'content') else str(msg)\n",
    "            outputs[msg.name] = content\n",
    "    return outputs\n",
    "\n",
    "def get_context_for_step(state, step: Dict) -> str:\n",
    "    \"\"\"Get context from previous agent output for the current step.\n",
    "    Uses agent_outputs state field for efficient access.\"\"\"\n",
    "    uses_data_from = step.get(\"uses_data_from\", [])\n",
    "    if not uses_data_from:\n",
    "        return \"\"\n",
    "    \n",
    "    agent_outputs = state.get(\"agent_outputs\", {})\n",
    "    plan = state.get(\"plan\", {})\n",
    "    steps = plan.get(\"steps\", [])\n",
    "    \n",
    "    context_parts = []\n",
    "    for step_num in uses_data_from:\n",
    "        # Find the agent that produced that step\n",
    "        for s in steps:\n",
    "            if s.get(\"step_number\") == step_num:\n",
    "                agent_name = s.get(\"agent\")\n",
    "                if agent_name in agent_outputs:\n",
    "                    output = agent_outputs[agent_name]\n",
    "                    # Truncate if too long\n",
    "                    if len(output) > 2000:\n",
    "                        output = output[:2000] + \"...\"\n",
    "                    context_parts.append(f\"From {agent_name}: {output}\")\n",
    "                break\n",
    "    \n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "print(\"‚úÖ All helper functions defined:\")\n",
    "print(\"   - get_latest_human_message()\")\n",
    "print(\"   - has_plan()\")\n",
    "print(\"   - get_current_step()\")\n",
    "print(\"   - is_plan_complete()\")\n",
    "print(\"   - get_all_agent_outputs()\")\n",
    "print(\"   - get_context_for_step()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_agent_outputs_for_synthesis(outputs: Dict[str, str], plan: Dict) -> str:\n",
    "    \"\"\"Format agent outputs for synthesis - efficient and clean.\n",
    "    \n",
    "    Uses outputs from state.agent_outputs (not message parsing).\n",
    "    Includes consolidated query info from plan for context.\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    for step in plan.get(\"steps\", []):\n",
    "        agent_name = step.get(\"agent\")\n",
    "        if agent_name in outputs:\n",
    "            output = outputs[agent_name]\n",
    "            # Truncate very long outputs to prevent token bloat\n",
    "            if len(output) > 5000:\n",
    "                output = output[:5000] + \"\\n... [truncated for brevity]\"\n",
    "            \n",
    "            formatted.append(f\"\"\"\n",
    "**{agent_name}** (Step {step.get('step_number')}/{plan.get('total_steps', len(plan.get('steps', [])))})\n",
    "Purpose: {step.get('purpose', 'N/A')}\n",
    "Query: {step.get('consolidated_query', 'N/A')[:100]}...\n",
    "\n",
    "Results:\n",
    "{output}\n",
    "\"\"\")\n",
    "    \n",
    "    if not formatted:\n",
    "        return \"No agent outputs available.\"\n",
    "    \n",
    "    return \"\\n\".join(formatted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Node Functions\n",
    "\n",
    "Now we define the **node functions** - the actual work units of our graph. Each node:\n",
    "- Receives the current state\n",
    "- Performs some action (LLM call, agent invocation, etc.)\n",
    "- Returns state updates (new messages to add)\n",
    "\n",
    "### Node Types:\n",
    "1. **Supervisor Node**: Handles routing AND synthesis (dual-purpose)\n",
    "2. **Agent Nodes**: Invoke specialized Cortex agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EFFICIENCY OPTIMIZATION: Removed LLM-based context extraction\n",
    "# Context is now passed directly via state.agent_outputs without extra LLM calls\n",
    "# This eliminates redundant supervisor re-entry and LLM invocations\n",
    "\n",
    "def supervisor_node(state: State) -> Command[Literal[\"CONTENT_AGENT\", \"DATA_ANALYST_AGENT\", \"RESEARCH_AGENT\", \"__end__\"]]:\n",
    "    \"\"\"\n",
    "    Supervisor node with EFFICIENCY OPTIMIZATIONS:\n",
    "    - ONE planning phase (no re-planning)\n",
    "    - Direct agent chaining (agents route to next agent, not back to supervisor)\n",
    "    - Context passed via state.agent_outputs (no LLM extraction)\n",
    "    - Only called for: 1) Planning, 2) Final synthesis\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    plan = state.get(\"plan\")\n",
    "    \n",
    "    # ========================================\n",
    "    # MODE 1: PLANNING (only runs ONCE)\n",
    "    # ========================================\n",
    "    if not has_plan(state):\n",
    "        latest_message = get_latest_human_message(messages)\n",
    "        \n",
    "        if not latest_message:\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"plan\": {\"steps\": [], \"plan_summary\": \"No query\"},\n",
    "                    \"planning_complete\": True\n",
    "                },\n",
    "                goto=\"__end__\"\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            planning_chain = planning_prompt_template | supervisor_model\n",
    "            response = planning_chain.invoke({\"input\": latest_message})\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            # Parse JSON plan\n",
    "            if \"{\" in content and \"}\" in content:\n",
    "                start = content.find(\"{\")\n",
    "                end = content.rfind(\"}\") + 1\n",
    "                plan = json.loads(content[start:end])\n",
    "            else:\n",
    "                raise ValueError(\"No valid JSON found\")\n",
    "            \n",
    "            # Display clean plan summary\n",
    "            print(f\"\\n{'‚îÅ'*60}\")\n",
    "            print(\"üìã EXECUTION PLAN (IMMUTABLE)\")\n",
    "            print(f\"{'‚îÅ'*60}\")\n",
    "            print(f\"\\n{plan.get('plan_summary', 'N/A')}\")\n",
    "            print(f\"\\nüìç Steps ({plan.get('total_steps', len(plan.get('steps', [])))} total):\")\n",
    "            for step in plan.get(\"steps\", []):\n",
    "                next_agent = step.get('next_agent', 'SYNTHESIS')\n",
    "                print(f\"   {step.get('step_number')}. {step.get('agent')} ‚Üí {next_agent}\")\n",
    "                print(f\"      Purpose: {step.get('purpose', 'N/A')[:50]}...\")\n",
    "            print(f\"{'‚îÅ'*60}\\n\")\n",
    "            \n",
    "            # Get first agent - plan will NOT be modified after this\n",
    "            first_step = plan.get(\"steps\", [{}])[0] if plan.get(\"steps\") else {}\n",
    "            first_agent = first_step.get(\"agent\", \"CONTENT_AGENT\")\n",
    "            \n",
    "            return Command(\n",
    "                update={\n",
    "                    \"plan\": plan,\n",
    "                    \"current_step\": 0,\n",
    "                    \"planning_complete\": True,  # Lock the plan\n",
    "                    \"agent_outputs\": {},\n",
    "                    \"execution_errors\": []\n",
    "                },\n",
    "                goto=first_agent\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Planning error: {e}\")\n",
    "            fallback_plan = {\n",
    "                \"plan_summary\": \"Direct query routing (planning failed)\",\n",
    "                \"total_steps\": 1,\n",
    "                \"steps\": [{\"step_number\": 1, \"agent\": \"CONTENT_AGENT\", \"purpose\": \"Handle query\", \"next_agent\": None}],\n",
    "                \"combination_strategy\": \"Present agent response directly\"\n",
    "            }\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"plan\": fallback_plan,\n",
    "                    \"current_step\": 0,\n",
    "                    \"planning_complete\": True,\n",
    "                    \"agent_outputs\": {},\n",
    "                    \"execution_errors\": []\n",
    "                },\n",
    "                goto=\"CONTENT_AGENT\"\n",
    "            )\n",
    "    \n",
    "    # ========================================\n",
    "    # MODE 2: ROUTING (route to next agent based on plan)\n",
    "    # ========================================\n",
    "    # Check if there are more steps to execute\n",
    "    if not is_plan_complete(state):\n",
    "        current_step = get_current_step(state)\n",
    "        if current_step:\n",
    "            next_agent = current_step.get(\"agent\")\n",
    "            if next_agent and next_agent in AGENT_NAMES:\n",
    "                print(f\"   ‚Üí Routing to {next_agent}\")\n",
    "                return Command(goto=next_agent)\n",
    "    \n",
    "    # ========================================\n",
    "    # MODE 3: SYNTHESIS (when plan is complete)\n",
    "    # ========================================\n",
    "    original_question = get_latest_human_message(messages)\n",
    "    agent_outputs = get_all_agent_outputs(state)\n",
    "    execution_errors = state.get(\"execution_errors\", [])\n",
    "    \n",
    "    print(f\"üìä Synthesizing results from {len(agent_outputs)} agent(s)...\")\n",
    "    if execution_errors:\n",
    "        print(f\"   ‚ö†Ô∏è {len(execution_errors)} error(s) occurred during execution\")\n",
    "    \n",
    "    try:\n",
    "        formatted_outputs = format_agent_outputs_for_synthesis(agent_outputs, plan)\n",
    "        \n",
    "        # Add error summary if any\n",
    "        if execution_errors:\n",
    "            formatted_outputs += f\"\\n\\n**Execution Notes:**\\n- {len(execution_errors)} non-critical error(s) occurred\\n\"\n",
    "        \n",
    "        synthesis_chain = synthesis_prompt_template | supervisor_model\n",
    "        response = synthesis_chain.invoke({\n",
    "            \"question\": original_question,\n",
    "            \"plan_summary\": plan.get(\"plan_summary\", \"\"),\n",
    "            \"step_dependencies\": \"Data flows according to plan\",\n",
    "            \"combination_strategy\": plan.get(\"combination_strategy\", \"\"),\n",
    "            \"expected_final_output\": plan.get(\"expected_final_output\", \"Comprehensive analysis\"),\n",
    "            \"agent_outputs\": formatted_outputs\n",
    "        })\n",
    "        \n",
    "        content = response.content if hasattr(response, 'content') else str(response)\n",
    "        print(f\"‚úÖ Analysis complete\\n\")\n",
    "        \n",
    "        return Command(\n",
    "            update={\"messages\": [AIMessage(content=content, name=\"supervisor\")]},\n",
    "            goto=\"__end__\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Synthesis error: {e}\")\n",
    "        raw_outputs = \"\\n\\n\".join([f\"**{k}**:\\n{v[:2000]}\" for k, v in agent_outputs.items()])\n",
    "        return Command(\n",
    "            update={\"messages\": [AIMessage(content=f\"Analysis completed:\\n\\n{raw_outputs}\", name=\"supervisor\")]},\n",
    "            goto=\"__end__\"\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ supervisor_node() optimized - no re-planning, no LLM context extraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_with_context(original_query: str, state: State) -> str:\n",
    "    \"\"\"Build query with context from previous agent outputs (no LLM extraction needed).\"\"\"\n",
    "    current_step = get_current_step(state)\n",
    "    if not current_step:\n",
    "        return original_query\n",
    "    \n",
    "    # Get context directly from state\n",
    "    context = get_context_for_step(state, current_step)\n",
    "    if context:\n",
    "        return f\"{original_query}\\n\\nContext from previous analysis:\\n{context}\"\n",
    "    return original_query\n",
    "\n",
    "\n",
    "def content_agent_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    \"\"\"Content Agent node - handles customer feedback and sentiment analysis.\n",
    "    \n",
    "    Always routes back to supervisor for clean hub-and-spoke architecture.\n",
    "    Supervisor handles routing to next agent based on plan.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    query = get_latest_human_message(messages)\n",
    "    current_step_idx = state.get(\"current_step\", 0)\n",
    "    agent_outputs = state.get(\"agent_outputs\", {}).copy()\n",
    "    execution_errors = state.get(\"execution_errors\", []).copy()\n",
    "    \n",
    "    # Build query with context from previous steps\n",
    "    enhanced_query = build_query_with_context(query, state)\n",
    "    \n",
    "    print(f\"üîç CONTENT_AGENT analyzing...\")\n",
    "    \n",
    "    try:\n",
    "        result = content_agent.invoke(enhanced_query)\n",
    "        response_content = result.get(\"output\", \"\")\n",
    "        print(f\"   ‚úì Complete ({len(response_content)} chars)\")\n",
    "        \n",
    "        # Store in dedicated state field for efficient access\n",
    "        agent_outputs[\"CONTENT_AGENT\"] = response_content\n",
    "        \n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [AIMessage(content=response_content, name=\"CONTENT_AGENT\")],\n",
    "                \"current_step\": current_step_idx + 1,\n",
    "                \"agent_outputs\": agent_outputs\n",
    "            },\n",
    "            goto=\"supervisor\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"CONTENT_AGENT error: {str(e)}\"\n",
    "        print(f\"   ‚úó {error_msg}\")\n",
    "        execution_errors.append(error_msg)\n",
    "        \n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [AIMessage(content=f\"Error occurred: {str(e)}\", name=\"CONTENT_AGENT\")],\n",
    "                \"current_step\": current_step_idx + 1,\n",
    "                \"agent_outputs\": agent_outputs,\n",
    "                \"execution_errors\": execution_errors\n",
    "            },\n",
    "            goto=\"supervisor\"\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ content_agent_node() defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_agent_stream_response(chunks: List[str]) -> tuple[str, List[str]]:\n",
    "    \"\"\"Parse streaming response from Cortex Agent.\n",
    "    \n",
    "    EFFICIENCY OPTIMIZATION: Aggregates errors instead of appending each one.\n",
    "    Returns (content, errors) tuple.\n",
    "    \"\"\"\n",
    "    response_parts = []\n",
    "    errors = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            chunk_data = json.loads(chunk)\n",
    "            if isinstance(chunk_data, dict):\n",
    "                if chunk_data.get(\"type\") == \"text\":\n",
    "                    response_parts.append(chunk_data.get(\"text\", \"\"))\n",
    "                elif \"content\" in chunk_data:\n",
    "                    response_parts.append(str(chunk_data.get(\"content\", \"\")))\n",
    "                elif \"message\" in chunk_data:\n",
    "                    # Collect error but don't append to output (avoids cascading)\n",
    "                    errors.append(chunk_data.get(\"message\", \"Unknown error\"))\n",
    "            else:\n",
    "                response_parts.append(str(chunk_data))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            # Only append non-JSON chunks that look like actual content\n",
    "            if chunk and not chunk.startswith(\"{\"):\n",
    "                response_parts.append(chunk)\n",
    "    \n",
    "    return \"\".join(response_parts), errors\n",
    "\n",
    "\n",
    "def data_analyst_agent_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    \"\"\"Data Analyst Agent node - handles metrics and analytics queries.\n",
    "    \n",
    "    Always routes back to supervisor for clean hub-and-spoke architecture.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    original_query = get_latest_human_message(messages)\n",
    "    current_step_idx = state.get(\"current_step\", 0)\n",
    "    agent_outputs = state.get(\"agent_outputs\", {}).copy()\n",
    "    execution_errors = state.get(\"execution_errors\", []).copy()\n",
    "    \n",
    "    # Build query with context from previous steps\n",
    "    query = build_query_with_context(original_query, state)\n",
    "    \n",
    "    print(f\"üìä DATA_ANALYST_AGENT analyzing...\")\n",
    "    \n",
    "    try:\n",
    "        chunks = []\n",
    "        for chunk in data_analyst_agent.stream(query):\n",
    "            chunks.append(str(chunk))\n",
    "        \n",
    "        # Parse response with aggregated error handling\n",
    "        response_content, stream_errors = parse_agent_stream_response(chunks)\n",
    "        \n",
    "        # Report aggregated errors once (not per-chunk)\n",
    "        if stream_errors:\n",
    "            unique_errors = list(set(stream_errors))  # Deduplicate\n",
    "            if len(unique_errors) > 0:\n",
    "                print(f\"   ‚ö†Ô∏è {len(unique_errors)} unique error(s) during streaming\")\n",
    "                execution_errors.extend([f\"DATA_ANALYST streaming: {e}\" for e in unique_errors[:3]])  # Cap at 3\n",
    "        \n",
    "        print(f\"   ‚úì Complete ({len(response_content)} chars)\")\n",
    "        \n",
    "        # Store in dedicated state field\n",
    "        agent_outputs[\"DATA_ANALYST_AGENT\"] = response_content\n",
    "        \n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [AIMessage(content=response_content, name=\"DATA_ANALYST_AGENT\")],\n",
    "                \"current_step\": current_step_idx + 1,\n",
    "                \"agent_outputs\": agent_outputs,\n",
    "                \"execution_errors\": execution_errors\n",
    "            },\n",
    "            goto=\"supervisor\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"DATA_ANALYST_AGENT error: {str(e)}\"\n",
    "        print(f\"   ‚úó {error_msg}\")\n",
    "        execution_errors.append(error_msg)\n",
    "        \n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [AIMessage(content=f\"Error occurred: {str(e)}\", name=\"DATA_ANALYST_AGENT\")],\n",
    "                \"current_step\": current_step_idx + 1,\n",
    "                \"agent_outputs\": agent_outputs,\n",
    "                \"execution_errors\": execution_errors\n",
    "            },\n",
    "            goto=\"supervisor\"\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ data_analyst_agent_node() defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_agent_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    \"\"\"Research Agent node - handles market and strategic analysis.\n",
    "    \n",
    "    Always routes back to supervisor for clean hub-and-spoke architecture.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    original_query = get_latest_human_message(messages)\n",
    "    current_step_idx = state.get(\"current_step\", 0)\n",
    "    agent_outputs = state.get(\"agent_outputs\", {}).copy()\n",
    "    execution_errors = state.get(\"execution_errors\", []).copy()\n",
    "    \n",
    "    # Build query with context from previous steps\n",
    "    query = build_query_with_context(original_query, state)\n",
    "    \n",
    "    print(f\"üî¨ RESEARCH_AGENT analyzing...\")\n",
    "    \n",
    "    try:\n",
    "        chunks = []\n",
    "        for chunk in research_agent.stream(query):\n",
    "            chunks.append(str(chunk))\n",
    "        \n",
    "        # Parse response with aggregated error handling\n",
    "        response_content, stream_errors = parse_agent_stream_response(chunks)\n",
    "        \n",
    "        # Report aggregated errors once (not per-chunk)\n",
    "        if stream_errors:\n",
    "            unique_errors = list(set(stream_errors))\n",
    "            if len(unique_errors) > 0:\n",
    "                print(f\"   ‚ö†Ô∏è {len(unique_errors)} unique error(s) during streaming\")\n",
    "                execution_errors.extend([f\"RESEARCH streaming: {e}\" for e in unique_errors[:3]])\n",
    "        \n",
    "        print(f\"   ‚úì Complete ({len(response_content)} chars)\")\n",
    "        \n",
    "        # Store in dedicated state field\n",
    "        agent_outputs[\"RESEARCH_AGENT\"] = response_content\n",
    "        \n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [AIMessage(content=response_content, name=\"RESEARCH_AGENT\")],\n",
    "                \"current_step\": current_step_idx + 1,\n",
    "                \"agent_outputs\": agent_outputs,\n",
    "                \"execution_errors\": execution_errors\n",
    "            },\n",
    "            goto=\"supervisor\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"RESEARCH_AGENT error: {str(e)}\"\n",
    "        print(f\"   ‚úó {error_msg}\")\n",
    "        execution_errors.append(error_msg)\n",
    "        \n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [AIMessage(content=f\"Error occurred: {str(e)}\", name=\"RESEARCH_AGENT\")],\n",
    "                \"current_step\": current_step_idx + 1,\n",
    "                \"agent_outputs\": agent_outputs,\n",
    "                \"execution_errors\": execution_errors\n",
    "            },\n",
    "            goto=\"supervisor\"\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ research_agent_node() defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create the Routing Function\n",
    "\n",
    "The **routing function** is used by LangGraph's conditional edges. It:\n",
    "1. Reads the supervisor's routing decision (JSON)\n",
    "2. Parses the `next_agent` field\n",
    "3. Returns the name of the next node to execute\n",
    "\n",
    "This enables dynamic routing based on the supervisor's analysis of each query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing logic is now defined with edges in the graph setup cell\n",
    "print(\"‚úÖ Routing configured with edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Build the Workflow Graph\n",
    "\n",
    "Now we assemble all the pieces into a **StateGraph**. The graph defines:\n",
    "\n",
    "1. **Nodes**: The processing units (supervisor + 3 agents)\n",
    "2. **Edges**: The connections between nodes\n",
    "3. **Conditional Edges**: Dynamic routing based on state\n",
    "\n",
    "### Graph Structure:\n",
    "```\n",
    "START \n",
    "  ‚Üì\n",
    "supervisor (routing)\n",
    "  ‚Üì (conditional)\n",
    "  ‚îú‚îÄ‚îÄ CONTENT_AGENT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îú‚îÄ‚îÄ DATA_ANALYST_AGENT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ supervisor (synthesis)\n",
    "  ‚îî‚îÄ‚îÄ RESEARCH_AGENT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚Üì\n",
    "                                       END\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StateGraph with our State type\n",
    "workflow = StateGraph(State)\n",
    "print(\"‚úÖ StateGraph created\")\n",
    "\n",
    "# Add the supervisor node\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "print(\"   Added node: supervisor\")\n",
    "\n",
    "# Add the agent nodes\n",
    "workflow.add_node(\"CONTENT_AGENT\", content_agent_node)\n",
    "print(\"   Added node: CONTENT_AGENT\")\n",
    "\n",
    "workflow.add_node(\"DATA_ANALYST_AGENT\", data_analyst_agent_node)\n",
    "print(\"   Added node: DATA_ANALYST_AGENT\")\n",
    "\n",
    "workflow.add_node(\"RESEARCH_AGENT\", research_agent_node)\n",
    "print(\"   Added node: RESEARCH_AGENT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph edges\n",
    "# With Command routing, nodes specify their own destinations via goto\n",
    "\n",
    "# Only need the entry point - Command handles all other routing\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "print(\"‚úÖ Graph edges configured\")\n",
    "print(\"   Entry: START ‚Üí supervisor\")\n",
    "print(\"   Routing: Handled by Command.goto in each node\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Compile the Workflow\n",
    "\n",
    "Compiling the graph creates an executable application. The compiled graph:\n",
    "- Validates the graph structure\n",
    "- Creates an optimized execution plan\n",
    "- Returns a runnable object that can be invoked with input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the workflow into an executable application\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ Workflow compiled successfully!\")\n",
    "print(\"\\nüìä Workflow Summary (HUB-AND-SPOKE ARCHITECTURE):\")\n",
    "print(\"   ‚Ä¢ 1 Supervisor node (central coordinator)\")\n",
    "print(\"   ‚Ä¢ 3 Specialized agent nodes (spokes)\")\n",
    "print(\"   ‚Ä¢ Clean flat graph: all agents route through supervisor\")\n",
    "print(\"   ‚Ä¢ Flow: START ‚Üí supervisor ‚Üí Agent ‚Üí supervisor ‚Üí Agent ‚Üí supervisor ‚Üí END\")\n",
    "print(\"\\nüöÄ Efficiency Features:\")\n",
    "print(\"   ‚Ä¢ Immutable plan - created once, never modified during execution\")\n",
    "print(\"   ‚Ä¢ No LLM calls for routing - supervisor uses simple plan lookup\")\n",
    "print(\"   ‚Ä¢ Consolidated queries - single SQL aggregation instead of multiple calls\")\n",
    "print(\"   ‚Ä¢ Aggregated error handling - errors collected, not cascaded\")\n",
    "print(\"   ‚Ä¢ State-based context passing - no LLM calls for context extraction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Visualize the Graph (Optional)\n",
    "\n",
    "LangGraph can generate a visual representation of the workflow. This requires the `pygraphviz` or `grandalf` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to visualize the graph (requires graphviz)\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    \n",
    "    # Generate the graph visualization\n",
    "    graph_image = app.get_graph().draw_mermaid_png()\n",
    "    display(Image(graph_image))\n",
    "    print(\"‚úÖ Graph visualization generated\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è Graph visualization not available: {e}\")\n",
    "    print(\"   Install pygraphviz or grandalf for visualization support\")\n",
    "    print(\"\\n   Graph structure:\")\n",
    "    print(\"   START ‚Üí supervisor ‚Üí [CONTENT_AGENT | DATA_ANALYST_AGENT | RESEARCH_AGENT] ‚Üí supervisor ‚Üí END\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Test the Workflow\n",
    "\n",
    "Now let's test the workflow with some sample queries! Each query type should be routed to a different agent:\n",
    "\n",
    "| Query Type | Expected Agent |\n",
    "|------------|----------------|\n",
    "| Customer feedback, sentiment, complaints | CONTENT_AGENT |\n",
    "| Metrics, behavior, churn, analytics | DATA_ANALYST_AGENT |\n",
    "| Market research, competition, strategy | RESEARCH_AGENT |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.langgraph import TruGraph\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "from trulens.providers.cortex import Cortex\n",
    "from trulens.core import Metric, Selector\n",
    "from trulens.core.run import Run, RunConfig\n",
    "import uuid\n",
    "\n",
    "print(\"‚úÖ TruLens dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect TruLens to Snowflake for observability\n",
    "# This uses the same session we created earlier\n",
    "\n",
    "sf_connector = SnowflakeConnector(snowpark_session=session)\n",
    "print(\"‚úÖ Snowflake connector created for TruLens\")\n",
    "\n",
    "# Initialize the Cortex provider for client-side evaluations\n",
    "# This provider will use Snowflake Cortex LLMs to compute custom metrics\n",
    "trace_eval_provider = Cortex(\n",
    "    model_engine=\"claude-opus-4-5\",\n",
    "    snowpark_session=session\n",
    ")\n",
    "print(\"‚úÖ Cortex evaluation provider initialized\")\n",
    "print(f\"   Model: {trace_eval_provider.model_engine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan Quality - Evaluates how well the supervisor creates execution plans\n",
    "f_plan_quality = Metric(\n",
    "    implementation=trace_eval_provider.plan_quality_with_cot_reasons,\n",
    "    name=\"Plan Quality\",\n",
    "    selectors={\n",
    "        \"trace\": Selector.select_trace(),\n",
    "    },\n",
    ")\n",
    "print(\"‚úÖ Plan Quality metric configured\")\n",
    "\n",
    "# Plan Adherence - Checks if the agent follows the execution plan\n",
    "f_plan_adherence = Metric(\n",
    "    implementation=trace_eval_provider.plan_adherence_with_cot_reasons,\n",
    "    name=\"Plan Adherence\",\n",
    "    selectors={\n",
    "        \"trace\": Selector.select_trace(),\n",
    "    },\n",
    ")\n",
    "print(\"‚úÖ Plan Adherence metric configured\")\n",
    "\n",
    "# Execution Efficiency - Measures workflow efficiency\n",
    "f_execution_efficiency = Metric(\n",
    "    implementation=trace_eval_provider.execution_efficiency_with_cot_reasons,\n",
    "    name=\"Execution Efficiency\",\n",
    "    selectors={\n",
    "        \"trace\": Selector.select_trace(),\n",
    "    },\n",
    ")\n",
    "print(\"‚úÖ Execution Efficiency metric configured\")\n",
    "\n",
    "# Logical Consistency - Verifies consistency across agent responses\n",
    "f_logical_consistency = Metric(\n",
    "    implementation=trace_eval_provider.logical_consistency_with_cot_reasons,\n",
    "    name=\"Logical Consistency\",\n",
    "    selectors={\n",
    "        \"trace\": Selector.select_trace(),\n",
    "    },\n",
    ")\n",
    "print(\"‚úÖ Logical Consistency metric configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Instrument with TruLens and Run Evaluation\n",
    "\n",
    "Now we wrap the LangGraph application directly with `TruGraph` and run evaluation with full LangGraph input states.\n",
    "\n",
    "The evaluation dataset contains actual LangGraph state dicts (with `messages` key) that are passed directly to `graph.invoke()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique app name and version\n",
    "APP_NAME = f\"Customer Intelligence Multi-Agent\"\n",
    "APP_VERSION = f\"V{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# Directly wrap the LangGraph graph with TruGraph\n",
    "tru_app = TruGraph(\n",
    "    app,  # The compiled StateGraph from Step 11\n",
    "    app_name=APP_NAME,\n",
    "    app_version=APP_VERSION,\n",
    "    main_method=app.invoke,  # Use graph's invoke method directly\n",
    "    connector=sf_connector,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ TruGraph instrumented app created\")\n",
    "print(f\"   App Name: {APP_NAME}\")\n",
    "print(f\"   App Version: {APP_VERSION}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all metrics to compute (server-side + client-side)\n",
    "metrics_to_compute = [\n",
    "    # Server-side metrics\n",
    "    \"answer_relevance\",\n",
    "    # Client-side metrics\n",
    "    f_plan_quality,\n",
    "    f_plan_adherence,\n",
    "    f_execution_efficiency,\n",
    "    f_logical_consistency,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! üéâ You've successfully built an **efficiency-optimized multi-agent supervisor workflow** using LangGraph and Snowflake Cortex. Here's what we accomplished:\n",
    "\n",
    "### Key Components Built:\n",
    "\n",
    "1. **Extended State Management**: Custom state with `messages`, `plan`, `agent_outputs`, and `execution_errors`\n",
    "2. **Supervisor Model**: `ChatSnowflake` for intelligent planning and synthesis (2 modes only)\n",
    "3. **Specialized Agents**: Three `SnowflakeCortexAgent` instances with direct chaining\n",
    "4. **Efficient Prompt Engineering**: Planning prompt emphasizing consolidated queries and clear tool separation\n",
    "5. **Optimized Node Functions**: Agents route directly to each other without supervisor re-entry\n",
    "6. **Graph Structure**: Linear execution with plan-based agent chaining\n",
    "\n",
    "### Efficiency-Optimized Architecture:\n",
    "\n",
    "```\n",
    "User Query ‚Üí Supervisor (Plan ONCE) ‚Üí Agent1 ‚Üí Agent2... ‚Üí Supervisor (Synthesize ONCE) ‚Üí Executive Summary\n",
    "```\n",
    "\n",
    "### Efficiency Improvements Implemented:\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| **Repeated planning/re-planning** | Immutable plan - created once, locked with `planning_complete` flag |\n",
    "| **Cascading error chains** | Aggregated error handling - errors collected and deduplicated, not appended per-chunk |\n",
    "| **Supervisor bottleneck** | Direct agent chaining via `next_agent` field in plan steps |\n",
    "| **Redundant tool invocations** | Planning prompt emphasizes consolidated SQL queries (one aggregation vs multiple) |\n",
    "| **Duplicated invocations** | Supervisor only called twice (plan + synthesize), not between each agent |\n",
    "| **Overlapping tooling** | Clear tool separation - each agent has ONE tool with distinct data sources |\n",
    "\n",
    "### Planning Features:\n",
    "\n",
    "The supervisor creates a **single, immutable execution plan** that includes:\n",
    "- **`next_agent` Field**: Enables direct agent-to-agent routing\n",
    "- **`consolidated_query`**: Encourages single queries instead of multiple separate calls\n",
    "- **Clear Tool Boundaries**: CONTENT_AGENT (cortex_search), DATA_ANALYST_AGENT (cortex_analyst on metrics), RESEARCH_AGENT (cortex_analyst on analytics)\n",
    "- **`total_steps`**: Explicit step count for progress tracking\n",
    "\n",
    "### Error Handling Strategy:\n",
    "\n",
    "- Errors collected in `state.execution_errors` array (not cascaded in output)\n",
    "- Unique errors deduplicated before storage\n",
    "- Graceful degradation - agents continue to next step even on partial errors\n",
    "- Summary synthesis includes error count without verbose per-chunk messages\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Add more agents**: Extend the workflow with additional specialized agents\n",
    "- **Implement memory**: Add conversation persistence with LangGraph checkpointers\n",
    "- **Add human-in-the-loop**: Include plan approval steps for critical decisions\n",
    "- **Deploy to production**: Use LangGraph Cloud or LangGraph Studio for deployment\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [Snowflake Cortex Documentation](https://docs.snowflake.com/en/user-guide/snowflake-cortex)\n",
    "- [LangChain Snowflake Integration](https://python.langchain.com/docs/integrations/providers/snowflake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define evaluation inputs as full LangGraph state dicts\n",
    "# This passes the actual state that LangGraph expects directly to graph.invoke()\n",
    "evaluation_inputs = [\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Assess the churn risk for customers complaining about API issues.\")],\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What industries have the highest customer lifetime value and represent our best strategic expansion opportunities?\")],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create DataFrame with the state dicts\n",
    "queries_df = pd.DataFrame({\"input_state\": evaluation_inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique run name\n",
    "run_name = f\"customer_intel_eval_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# Configure the evaluation run\n",
    "# The dataset_spec maps to the column containing full LangGraph state dicts\n",
    "run_config = RunConfig(\n",
    "    run_name=run_name,\n",
    "    dataset_name=\"customer_intelligence_queries\",\n",
    "    source_type=\"DATAFRAME\",\n",
    "    dataset_spec={\"RECORD_ROOT.INPUT\": \"input_state\"},  # Maps to state dict column\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Run configuration created\")\n",
    "print(f\"   Run Name: {run_name}\")\n",
    "print(f\"   Dataset: customer_intelligence_queries\")\n",
    "print(f\"   Source: DataFrame with {len(queries_df)} input states\")\n",
    "\n",
    "# Add the run to the instrumented app\n",
    "run: Run = tru_app.add_run(run_config=run_config)\n",
    "print(f\"\\n‚úÖ Run added to TruGraph app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Starting evaluation run...\")\n",
    "print(f\"   Processing {len(queries_df)} queries through the multi-agent workflow\\n\")\n",
    "\n",
    "run.start(input_df=queries_df)\n",
    "\n",
    "print(\"‚úÖ Run started!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ============================================\n",
    "# Wait for invocations to complete\n",
    "# ============================================\n",
    "print(\"‚è≥ Waiting for invocations to complete...\")\n",
    "print(f\"   Initial Status: {run.get_status()}\")\n",
    "\n",
    "wait_count = 0\n",
    "max_wait = 60  # Max 5 minutes (60 * 5 seconds)\n",
    "\n",
    "while run.get_status() != \"INVOCATION_COMPLETED\" and wait_count < max_wait:\n",
    "    time.sleep(5)\n",
    "    wait_count += 1\n",
    "    status = run.get_status()\n",
    "    if wait_count % 6 == 0:  # Print status every 30 seconds\n",
    "        print(f\"   [{wait_count * 5}s] Status: {status}\")\n",
    "\n",
    "final_status = run.get_status()\n",
    "print(f\"\\n‚úÖ Final Status: {final_status}\")\n",
    "\n",
    "if final_status == \"INVOCATION_COMPLETED\":\n",
    "    print(\"   All invocations completed successfully!\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Run did not complete. Current status: {final_status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Compute evaluation metrics\n",
    "# ============================================\n",
    "run.compute_metrics(metrics_to_compute)\n",
    "\n",
    "print(\"‚úÖ Metrics computation initiated!\")\n",
    "print(f\"   Current Status: {run.get_status()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
